{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of BERT Base vocabulary: 30522\n"
     ]
    }
   ],
   "source": [
    "# load the bert base uncased tokneizer : 12 encode\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(f'Length of BERT Base vocabulary: {len(tokenizer.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1037, 3722, 6251, 102]\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "text = 'A simple sentence'\n",
    "\n",
    "tokens =tokenizer.encode(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 18:31:11.087554: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-25 18:31:15.339780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] a simple sentence [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# back a words\n",
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2026, 2767, 2409, 2033, 2055, 2023, 2465, 1998, 1045, 2293, 2009, 2061, 2521, 999, 2016, 2001, 2157, 102]\n"
     ]
    }
   ],
   "source": [
    "# sample 2\n",
    "\n",
    "text = 'My friend told me about this class and i love it so far! she was right'\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text My friend told me about this class and i love it so far! she was right , Num tokens 19\n",
      "Tokens 101, subword: [CLS]\n",
      "Tokens 2026, subword: my\n",
      "Tokens 2767, subword: friend\n",
      "Tokens 2409, subword: told\n",
      "Tokens 2033, subword: me\n",
      "Tokens 2055, subword: about\n",
      "Tokens 2023, subword: this\n",
      "Tokens 2465, subword: class\n",
      "Tokens 1998, subword: and\n",
      "Tokens 1045, subword: i\n",
      "Tokens 2293, subword: love\n",
      "Tokens 2009, subword: it\n",
      "Tokens 2061, subword: so\n",
      "Tokens 2521, subword: far\n",
      "Tokens 999, subword: !\n",
      "Tokens 2016, subword: she\n",
      "Tokens 2001, subword: was\n",
      "Tokens 2157, subword: right\n",
      "Tokens 102, subword: [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(f'Text {text} , Num tokens {(len(tokens))}')\n",
    "\n",
    "for t in tokens:\n",
    "    print(f'Tokens {t}, subword: {tokenizer.decode([t])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check not in words\n",
    "'sinan' in tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 101, subword: [CLS]\n",
      "Tokens: 8254, subword: sin\n",
      "Tokens: 2319, subword: ##an\n",
      "Tokens: 7459, subword: loves\n",
      "Tokens: 1037, subword: a\n",
      "Tokens: 3376, subword: beautiful\n",
      "Tokens: 2154, subword: day\n",
      "Tokens: 102, subword: [SEP]\n"
     ]
    }
   ],
   "source": [
    "text_with_unkown_words = 'Sinan loves a beautiful day'\n",
    "tokens_with_unkown_words = tokenizer.encode(text_with_unkown_words)\n",
    "\n",
    "for t in tokens_with_unkown_words:\n",
    "    print(f'Tokens: {t}, subword: {tokenizer.decode([t])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 8254, 2319, 102]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('sinan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2019, 102]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('an')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 101, subword: [CLS]\n",
      "Tokens: 8254, subword: sin\n",
      "Tokens: 2319, subword: ##an\n",
      "Tokens: 2003, subword: is\n",
      "Tokens: 2256, subword: our\n",
      "Tokens: 9450, subword: instructor\n",
      "Tokens: 2005, subword: for\n",
      "Tokens: 2023, subword: this\n",
      "Tokens: 12476, subword: awesome\n",
      "Tokens: 6499, subword: ##so\n",
      "Tokens: 18796, subword: ##uce\n",
      "Tokens: 1999, subword: in\n",
      "Tokens: 2465, subword: class\n",
      "Tokens: 102, subword: [SEP]\n"
     ]
    }
   ],
   "source": [
    "text_with_unkown_words = 'Sinan is our instructor for this awesomesouce in class'\n",
    "tokens_with_unkown_words = tokenizer.encode(text_with_unkown_words)\n",
    "\n",
    "for t in tokens_with_unkown_words:\n",
    "    print(f'Tokens: {t}, subword: {tokenizer.decode([t])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2026, 2767, 2409, 2033, 2055, 2023, 2465, 1998, 1045, 2293, 2009, 2061, 2521, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "text = 'My friend told me about this class and i love it so far'\n",
    "\n",
    "# encode_plus gives us token ids, attention mask and segment ids (A VS B) ex I like you vs i like you and mom (give a padding)\n",
    "tokens = tokenizer.encode_plus(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_pet = tokenizer.encode(' I love my pet')\n",
    "python_language = tokenizer.encode(' I love my python language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextual embedding\n",
    "python_pet_embedding = model(torch.tensor(python_pet).unsqueeze(0))[0][:, 5, :].detach().numpy()\n",
    "\n",
    "python_language_embedding = model(torch.tensor(python_language).unsqueeze(0))[0][:, 5, :].detach().numpy()\n",
    "\n",
    "snake_alone_embedding = model(torch.tensor(tokenizer.encode('snake')).unsqueeze(0))[0][:, 1, :].detach().numpy()\n",
    "\n",
    "programming_embedding = model(torch.tensor(tokenizer.encode('programming')).unsqueeze(0))[0][:, 1, :].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_pet_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_language_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53738034]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(python_language_embedding, snake_alone_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01107592]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(python_pet_embedding, programming_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
